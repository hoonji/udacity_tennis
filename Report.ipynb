{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530268fa",
   "metadata": {},
   "source": [
    "# Project 3: Collaboration and Competition\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project solves the multi agent Tennis environment using the PPO algorithm [1].\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### Background\n",
    "\n",
    "The project uses Proximal Policy Optimization [1], which maximizes the surrogate objective function\n",
    "\n",
    "$$\\sum_tE_{s_t \\sim p_{\\theta}(s_t)}[E_{a_t \\sim \\pi_{\\theta}(a_t|s_t)}[\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t)]]$$\n",
    "\n",
    "which is roughly equivalent to maximizing the reinforcement learning objective [3]\n",
    "\n",
    "$$E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t}\\gamma^tr(s_t,a_t)]$$\n",
    "given that $\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)} \\le \\epsilon$ [4].\n",
    "\n",
    "PPO keeps the new policies similar to the old by conditionallly clipping the ratio $\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)}$:\n",
    "\n",
    "$$\\min(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a), clip(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a))$$\n",
    "\n",
    "SGD is performed on the above objective with respect to model parameters Î¸ to maximize the agent's performance.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "Orthogonal initialization of weights [5] was adopted for stable DNN training. Entropy regularization [6] was initially used (as it improved performance on the Reacher environment), but turned out to be catastrophic for the training performance as the initial sparse rewards would cause the agents to increase their standard deviation parameters indefinitely.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "```\n",
    "LEARNING_RATE = 3e-4\n",
    "ADAM_EPS = 1e-5\n",
    "GAMMA = .99\n",
    "LAMBDA = .95\n",
    "UPDATE_EPOCHS = 3\n",
    "N_MINIBATCHES = 10\n",
    "CLIP_COEF = .2\n",
    "MAX_GRAD_NORM = 5\n",
    "GAE_LAMBDA = .95\n",
    "V_COEF = .5\n",
    "HIDDEN_LAYER_SIZE = 32\n",
    "ROLLOUT_LEN = 1024\n",
    "N_ROLLOUTS = 50000\n",
    "ENTROPY_COEF = 0\n",
    "```\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "The model consists of two networks, one actor network and one critic network, that both consist of three fully connected layers, with hidden layers having size 32.\n",
    "\n",
    "The actor network takes in inputs of size (n_batch, n_observations) and outputs values of size (n_batch, n_actions) with each value between -1 and 1. The values represent the means of each of the two action components and are used to sample action values from a normal distribution. The actor network uses Tanh activations for both the initial layers and the output to scale values between -1 and 1.\n",
    "\n",
    "The critic network also takes in inputs of size (n_batch, n_observations), but outputs values of size (n_batch, 1) that represent the predicted value of the observation. The critic net also uses tanh activations between layers and does not require the last tanh layer.\n",
    "\n",
    "## Learning curve\n",
    "\n",
    "The agents achieved an average score (over 100 episodes) of .5 in 45217 total episodes. As training continued to be stable after this point, I trained the agents for longer to reach an average score of 1.0.\n",
    "\n",
    "![](learning_curve.png?1)\n",
    "\n",
    "## Future work\n",
    "\n",
    "- Different network architectures and network sizes can be experimented with to find the best fit for the current problem. Batch normalization may also help stabilize learning.\n",
    "- Experimenting with a different suitable entropy coefficient may improve training performance.\n",
    "- Other training related hyperparameters (update epochs, minibatch size, rollout length, etc.) can be reconfigured until an optimal combination for this environment is found.\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] https://arxiv.org/pdf/1707.06347.pdf Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n",
    "- [3] https://www.youtube.com/watch?v=ySenCHPsKJU&list=PL_iWQOsE6TfX7MaC6C3HcdOf1g337dlC9&index=38&ab_channel=RAIL Berkeley CS285 Lecture 9, Part 1\n",
    "- [4] https://www.youtube.com/watch?v=ySenCHPsKJU&list=PL_iWQOsE6TfX7MaC6C3HcdOf1g337dlC9&index=38&ab_channel=RAIL Berkeley CS285 Lecture 9, Part 2\n",
    "- [5] https://openreview.net/forum?id=r1etN1rtPB Logan, Engstrom, Ilyas Andrew, Santurkar Shibani, Tsipras Dimitris, Janoos Firdaus, Rudolph Larry, and Madry Aleksander. \"Implementation matters in deep RL: A case study on PPO and TRPO.\" In International Conference on Learning Representations. 2019.\n",
    "- [6] http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com Mnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. \"Asynchronous methods for deep reinforcement learning.\" In International conference on machine learning, pp. 1928-1937. PMLR, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
